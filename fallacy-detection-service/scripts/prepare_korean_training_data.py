"""
ì´ˆê¸° í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸
ì˜ì–´ ë°ì´í„°ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í•œêµ­ì–´ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ìƒì„±
"""
import sys
import os
import json
from datasets import load_dataset

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from app.models.translator import Translator
from config.settings import settings

def prepare_korean_training_data():
    """ì˜ì–´ ë°ì´í„°ì…‹ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
    translator = Translator()
    
    if not translator.enabled:
        print("âŒ OpenAI API keyê°€ ì—†ìŠµë‹ˆë‹¤. ë²ˆì—­ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ“¥ HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = load_dataset("tasksource/logical-fallacy", split="train")
    
    # ìƒ˜í”Œë§ (ì „ì²´ ë°ì´í„°ê°€ ë§ìœ¼ë¯€ë¡œ ì¼ë¶€ë§Œ ì‚¬ìš©)
    sample_size = min(2000, len(dataset))
    dataset = dataset.select(range(sample_size))
    
    print(f"âœ… {sample_size}ê°œ ìƒ˜í”Œ ì„ íƒ")
    print("ğŸ”„ ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ ì‹œì‘...")
    
    translated_data = []
    for i, item in enumerate(dataset):
        if i % 100 == 0:
            print(f"   ì§„í–‰ ì¤‘: {i}/{sample_size} ({i*100//sample_size}%)")
        
        english_text = item.get("text", item.get("argument", ""))
        label = item.get("label", item.get("logical_fallacy", "no_fallacy"))
        
        # í•œêµ­ì–´ë¡œ ë²ˆì—­
        korean_text = translator.translate_to_korean(english_text, "en")
        
        if korean_text:
            translated_data.append({
                "text": korean_text,
                "label": label
            })
        else:
            print(f"   âš ï¸  ë²ˆì—­ ì‹¤íŒ¨: ìƒ˜í”Œ {i} ê±´ë„ˆëœ€")
    
    print(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translated_data)}ê°œ ìƒ˜í”Œ")
    
    # ê²°ê³¼ ì €ì¥
    output_dir = "./data/korean_training"
    os.makedirs(output_dir, exist_ok=True)
    
    output_file = os.path.join(output_dir, "korean_training_data.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(translated_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ë²ˆì—­ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"ğŸ“Š ë¼ë²¨ ë¶„í¬:")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    label_counts = {}
    for item in translated_data:
        label = item["label"]
        label_counts[label] = label_counts.get(label, 0) + 1
    
    for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {label}: {count}ê°œ")
    
    print("\nâœ… ì´ˆê¸° í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ğŸ“ ë‹¤ìŒ ë‹¨ê³„: ì´ ë°ì´í„°ë¡œ í•œêµ­ì–´ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•˜ì„¸ìš”.")

if __name__ == "__main__":
    prepare_korean_training_data()

