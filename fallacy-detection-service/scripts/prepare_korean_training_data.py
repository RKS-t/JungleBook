"""
ì´ˆê¸° í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸
ì˜ì–´ ë°ì´í„°ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í•œêµ­ì–´ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ìƒì„±
"""
import sys
import os
import json
from datasets import load_dataset

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from app.models.translator import Translator
from config.settings import settings

def prepare_korean_training_data():
    """ì˜ì–´ ë°ì´í„°ì…‹ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
    translator = Translator()
    
    if not translator.enabled:
        print("âŒ OpenAI API keyê°€ ì—†ìŠµë‹ˆë‹¤. ë²ˆì—­ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ðŸ“¥ HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = load_dataset("tasksource/logical-fallacy", split="train")
    
    # ìƒ˜í”Œë§ (ì „ì²´ ë°ì´í„°ê°€ ë§Žìœ¼ë¯€ë¡œ ì¼ë¶€ë§Œ ì‚¬ìš©)
    sample_size = min(2000, len(dataset))
    dataset = dataset.select(range(sample_size))
    
    print(f"âœ… {sample_size}ê°œ ìƒ˜í”Œ ì„ íƒ")
    print("ðŸ”„ ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ ì‹œìž‘...")
    
    translated_data = []
    for i, item in enumerate(dataset):
        if i % 100 == 0:
            print(f"   ì§„í–‰ ì¤‘: {i}/{sample_size} ({i*100//sample_size}%)")
        
        # ì›ë³¸ ë°ì´í„°ì…‹ êµ¬ì¡°ì— ë§žê²Œ ìˆ˜ì •
        # tasksource/logical-fallacy ë°ì´í„°ì…‹ì€ logical_fallacies í‚¤ ì‚¬ìš©
        if "logical_fallacies" in item:
            # ë…¼ë¦¬ ì˜¤ë¥˜ íƒ€ìž… ì¶”ì¶œ
            fallacies = item["logical_fallacies"]
            if isinstance(fallacies, list) and len(fallacies) > 0:
                label = fallacies[0]  # ì²« ë²ˆì§¸ ë…¼ë¦¬ ì˜¤ë¥˜ íƒ€ìž… ì‚¬ìš©
            elif isinstance(fallacies, str) and fallacies.strip():
                label = fallacies.strip()
            else:
                label = "no_fallacy"
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (source_article ì‚¬ìš©)
            english_text = item.get("source_article", "")
            if not english_text:
                english_text = item.get("text", item.get("argument", ""))
        else:
            # ê¸°ì¡´ êµ¬ì¡° ì§€ì›
            english_text = item.get("text", item.get("argument", ""))
            label = item.get("label", item.get("logical_fallacy", "no_fallacy"))
        
        # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìžˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if not english_text or not english_text.strip():
            continue
        
        # í•œêµ­ì–´ë¡œ ë²ˆì—­ (ì‹¤ì œ ë²ˆì—­ ê²°ê³¼ë§Œ ì‚¬ìš©)
        korean_text = translator.translate_to_korean(english_text, "en")
        
        # ë²ˆì—­ ê²°ê³¼ê°€ ì‹¤ì œ ë²ˆì—­ì¸ì§€ í™•ì¸ (í”„ë¡¬í”„íŠ¸ ì‘ë‹µì´ ì•„ë‹Œì§€)
        if korean_text and len(korean_text) > 20 and "Please provide" not in korean_text and "ë²ˆì—­" not in korean_text[:50]:
            translated_data.append({
                "text": korean_text,
                "label": label
            })
        else:
            if i < 5:  # ì²˜ìŒ ëª‡ ê°œë§Œ ê²½ê³  ì¶œë ¥
                print(f"   âš ï¸  ë²ˆì—­ ì‹¤íŒ¨ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ì‘ë‹µ: ìƒ˜í”Œ {i} ê±´ë„ˆëœ€")
    
    print(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translated_data)}ê°œ ìƒ˜í”Œ")
    
    # ê²°ê³¼ ì €ìž¥
    output_dir = "./data/korean_training"
    os.makedirs(output_dir, exist_ok=True)
    
    output_file = os.path.join(output_dir, "korean_training_data.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(translated_data, f, ensure_ascii=False, indent=2)
    
    print(f"ðŸ’¾ ë²ˆì—­ëœ ë°ì´í„° ì €ìž¥ ì™„ë£Œ: {output_file}")
    print(f"ðŸ“Š ë¼ë²¨ ë¶„í¬:")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    label_counts = {}
    for item in translated_data:
        label = item["label"]
        label_counts[label] = label_counts.get(label, 0) + 1
    
    for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {label}: {count}ê°œ")
    
    print("\nâœ… ì´ˆê¸° í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ðŸ“ ë‹¤ìŒ ë‹¨ê³„: ì´ ë°ì´í„°ë¡œ í•œêµ­ì–´ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•˜ì„¸ìš”.")

if __name__ == "__main__":
    prepare_korean_training_data()

