#!/usr/bin/env python3
"""
í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- ì†ŒìŠ¤: MidhunKanadan/logical-fallacy-classification (HF)
- ì†ŒìŠ¤: MAFALDA (ê³µê°œ ì‹œ ì‚¬ìš©, í˜„ì¬ ë¹„ê³µê°œë©´ ê±´ë„ˆëœ€)
- ë¼ë²¨ ë§¤í•‘ â†’ ìš°ë¦¬ ìŠ¤í‚¤ë§ˆ
- í•˜ì´ë¸Œë¦¬ë“œ ìƒ˜í”Œë§:
  - ì ì€ ë¼ë²¨: ì˜¤ë²„ìƒ˜í”Œë§(min_count)
  - ë§ì€ ë¼ë²¨: ì–¸ë”ìƒ˜í”Œë§(max_count)
  - no_fallacy ë¶€ì¡± ì‹œ: í•©ì„± ë¬¸ì¥ìœ¼ë¡œ ë³´ê°•
- (ì„ íƒ) í•œêµ­ì–´ ë²ˆì—­ í›„ ì €ì¥
"""

import os
import sys
import json
import random
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from datasets import load_dataset, Dataset
from app.models.translator import Translator
from app.services.training_service import TrainingService
from config.settings import settings

# ìš°ë¦¬ ìŠ¤í‚¤ë§ˆ
TARGET_LABELS = [
    "ad_hominem",
    "straw_man",
    "false_dilemma",
    "appeal_to_emotion",
    "circular_reasoning",
    "hasty_generalization",
    "false_cause",
    "bandwagon",
    "appeal_to_authority",
    "red_herring",
    "no_fallacy",
]

# ì†ŒìŠ¤ ë¼ë²¨ â†’ ìš°ë¦¬ ë¼ë²¨ ë§¤í•‘
SOURCE_TO_TARGET = {
    # logical-fallacy-classification (ì˜ˆìƒ ë¼ë²¨)
    "ad hominem": "ad_hominem",
    "ad_hominem": "ad_hominem",
    "hasty generalization": "hasty_generalization",
    "hasty_generalization": "hasty_generalization",
    "appeal to emotion": "appeal_to_emotion",
    "appeal_to_emotion": "appeal_to_emotion",
    "appeal to authority": "appeal_to_authority",
    "appeal_to_authority": "appeal_to_authority",
    "appeal to popularity": "bandwagon",
    "appeal_to_popularity": "bandwagon",
    "bandwagon": "bandwagon",
    "false cause": "false_cause",
    "false_cause": "false_cause",
    "false dilemma": "false_dilemma",
    "false_dilemma": "false_dilemma",
    "straw man": "straw_man",
    "straw_man": "straw_man",
    "red herring": "red_herring",
    "red_herring": "red_herring",
    "circular reasoning": "circular_reasoning",
    "circular_reasoning": "circular_reasoning",
    "no fallacy": "no_fallacy",
    "no_fallacy": "no_fallacy",
    # MAFALDA ì˜ˆìƒ ë¼ë²¨ â†’ ìŠ¤í‚¤ë§ˆ ë§¤í•‘ (ì¶”ì •ì¹˜)
    "faulty generalization": "hasty_generalization",
    "ad populum": "bandwagon",
    "false causality": "false_cause",
    "fallacy of logic": "red_herring",          # í¬ê´„ ë¼ë²¨ â†’ red_herringë¡œ ê·€ì†
    "fallacy of relevance": "red_herring",
    "fallacy of extension": "straw_man",
    "fallacy of credibility": "appeal_to_authority",
    "equivocation": "red_herring",
    "intentional": "red_herring",
}

SYNTHETIC_NO_FALLACY = [
    "ì´ ì£¼ì¥ì€ ê·¼ê±°ì™€ ë…¼ë¦¬ë¥¼ ê°–ì¶”ê³  ìˆìœ¼ë©°, ëª…í™•í•œ ì¸ê³¼ê´€ê³„ë¥¼ ì œì‹œí•œë‹¤.",
    "ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì„ ê· í˜• ìˆê²Œ ì œì‹œí•˜ë©°, ê°ì •ì  í˜¸ì†Œ ì—†ì´ ë…¼ë¦¬ë¥¼ ì „ê°œí•œë‹¤.",
    "ì „ì œì™€ ê²°ë¡ ì´ ì¼ê´€ë˜ê³  ìˆœí™˜ ë…¼ì¦ì´ë‚˜ í—ˆìœ„ ì¸ê³¼ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.",
    "ìë£Œì™€ í†µê³„ë¥¼ ì¸ìš©í•˜ì—¬ ì£¼ì¥ì˜ ì‹ ë¢°ì„±ì„ ë’·ë°›ì¹¨í•˜ë©°, ì¼ë°˜í™” ì˜¤ë¥˜ë¥¼ í”¼í•œë‹¤.",
    "ë°˜ë¡€ì™€ í•œê³„ë¥¼ ì¸ì •í•˜ë©°, ê³¼ë„í•œ ë‹¨ì •ì´ë‚˜ ì¸ì‹ ê³µê²©ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.",
]


def load_hf_dataset(name: str, split: str = "train") -> List[Dict]:
    try:
        ds = load_dataset(name, split=split)
        return [dict(item) for item in ds]
    except Exception as e:
        print(f"âš ï¸  {name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


def map_record(text: str, label: str) -> Tuple[str, str]:
    mapped = SOURCE_TO_TARGET.get(label.strip().lower(), None)
    return text, mapped


def build_dataset() -> List[Dict]:
    records = []

    # 1) logical-fallacy-classification (train/dev/test ëª¨ë‘ ì‚¬ìš©)
    for split in ["train", "validation", "test"]:
        data = load_hf_dataset("MidhunKanadan/logical-fallacy-classification", split=split)
        for item in data:
            text = item.get("text", "") or item.get("claim", "")
            label = item.get("label", "") or item.get("fallacy_type", "")
            text, mapped = map_record(text, label)
            if text and mapped:
                records.append({"text": text, "label": mapped})

    # 2) MAFALDA (ê³µê°œ ì‹œ)
    mafalda = load_hf_dataset("ChadiHelwe/MAFALDA", split="train")
    for item in mafalda:
        text = item.get("source_article", "") or item.get("text", "")
        label = ""
        lf = item.get("logical_fallacies", "")
        if isinstance(lf, list) and lf:
            label = lf[0]
        elif isinstance(lf, str):
            label = lf
        text, mapped = map_record(text, label)
        if text and mapped:
            records.append({"text": text, "label": mapped})

    return records


def hybrid_sample(records: List[Dict], min_count: int = 300, max_count: int = 800) -> List[Dict]:
    by_label = defaultdict(list)
    for r in records:
        by_label[r["label"]].append(r)

    augmented = []
    for label in TARGET_LABELS:
        items = by_label.get(label, [])
        cnt = len(items)

        # no_fallacyê°€ ë¶€ì¡±í•˜ë©´ í•©ì„± ë°ì´í„°ë¡œ ë³´ê°•
        if label == "no_fallacy" and cnt < min_count:
            needed = min_count - cnt
            for i in range(needed):
                augmented.append({"text": random.choice(SYNTHETIC_NO_FALLACY), "label": "no_fallacy"})
            cnt += needed

        if cnt == 0:
            continue
        if cnt < min_count:
            # ì˜¤ë²„ìƒ˜í”Œë§
            k = min_count - cnt
            sampled = random.choices(items, k=k)
            augmented.extend(items + sampled)
        elif cnt > max_count:
            # ì–¸ë”ìƒ˜í”Œë§
            sampled = random.sample(items, k=max_count)
            augmented.extend(sampled)
        else:
            augmented.extend(items)

    random.shuffle(augmented)
    return augmented


def translate_if_needed(records: List[Dict]) -> List[Dict]:
    if not settings.TRANSLATION_ENABLED:
        return records
    translator = Translator()
    if not translator.enabled:
        print("âš ï¸  ë²ˆì—­ ë¹„í™œì„±í™” ë˜ëŠ” API í‚¤ ì—†ìŒ. ë²ˆì—­ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
        return records

    translated = []
    for i, r in enumerate(records, 1):
        if i % 200 == 0:
            print(f"  ë²ˆì—­ ì§„í–‰: {i}/{len(records)}")
        ko = translator.translate_to_korean(r["text"], "en")
        if ko:
            translated.append({"text": ko, "label": r["label"]})
    print(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translated)} / {len(records)}")
    return translated if translated else records


def save_dataset(records: List[Dict], path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ì €ì¥: {path} (ìƒ˜í”Œ {len(records)}ê°œ)")


def main():
    print("=" * 70)
    print("í†µí•© ë°ì´í„° ë¡œë“œ ë° ë§¤í•‘")
    print("=" * 70)

    records = build_dataset()
    if not records:
        print("âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ ê³µê°œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    print(f"ì›ë³¸ ë¡œë“œ/ë§¤í•‘ í›„ ìƒ˜í”Œ: {len(records)}ê°œ")

    # ë¼ë²¨ í†µê³„
    cnt = Counter([r["label"] for r in records])
    print("ë¼ë²¨ ë¶„í¬(ì›ë³¸):")
    for k, v in cnt.most_common():
        print(f"  - {k}: {v}")

    # í•˜ì´ë¸Œë¦¬ë“œ ìƒ˜í”Œë§
    sampled = hybrid_sample(records, min_count=300, max_count=800)
    cnt2 = Counter([r["label"] for r in sampled])
    print("\në¼ë²¨ ë¶„í¬(ìƒ˜í”Œë§ í›„):")
    for k, v in cnt2.most_common():
        print(f"  - {k}: {v}")

    # ë²ˆì—­ (ì˜µì…˜)
    final_records = translate_if_needed(sampled)

    # ì €ì¥
    out_path = "./data/combined_training/combined_training_data.json"
    save_dataset(final_records, out_path)

    # í•™ìŠµ ì‹¤í–‰
    print("\nëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    trainer = TrainingService()
    trainer.train_model(final_records, output_dir="./models/korean_trained_model")
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")


if __name__ == "__main__":
    main()

